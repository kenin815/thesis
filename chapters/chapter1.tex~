\section{研究動機}
隨著網路跟電腦的普及，電腦資訊愈來愈豐富，使得我們尋找所需的資料變成一個大問題。因此，我們需要一套好的檢索系統
（Retrieval System）幫助我們快速地瀏覽資訊，並從中找到有用的部分，在過去已有許多文字檢索系統與演算法被開發出來並應用於產業中，如 Google Search、Microsoft Bing Search、Yahoo
Search等。隨著錄影音設備的普及，語音文件量正蓬勃地增加當中，隨著線上影片、會議錄音、線上課程等網站的興起，語音資料量越來越多，因此如何在其中找到使用者感興趣的資料便成為重要的議題，即為語音數位內容檢索（Spoken Content Retrieval）~\cite{chelba2008retrieval, lee2005spoken}。相較於文字資訊檢索，語音資訊檢索面臨到更多的挑戰，如辨識錯誤、辨識訓練資料不足等問題，使得此問題更形困難。

更由於行動裝置的出現，使用者可以不受地形時間限制，隨時取得資訊，促使許多網路公司一一推出了用語音輸入來檢索文字資訊的系統，如Google
公司推出的語音檢索功能即可讓使用者在手機或瀏覽器的介面上以語音輸入，由 Google 將其辨識成文字後再於 Google 的搜尋引擎上檢索資訊。 Apple 公司推出的個人語音助理 Siri，也讓使用者能以十分自然的方式對 Siri說出想要查詢的查詢詞（Query），由 Siri 辨識後在網路上檢索，並將檢索結果分門別類整理好後呈現給使用者看。如上述所說的這類檢索系統是用語音輸入的查詢詞去檢索大量的文字資訊，此方法稱為人聲檢索（Voice
Search），和本論文所探討的語音數位內容檢索 （Spoken Content Retrieval）完全不同。

本論文所探討的語音數位內容檢索，是指由於網路上有大量的多媒體文件，如線上影片、會議錄音、線上課程、電視連續劇、演講等，而使用者也有搜尋這些多媒體文件的需求，此類允許使用者用文字或聲音輸入查詢詞並搜尋語音數位內容（Spoken Content）的系統稱為語音數位內容檢索，如 TED（美國著名的演講網站）會將網站上的演講內容轉寫（Transcript） 成為文字，並允許使用者於網站上輸入文字檢索這些影片的文字稿。 Youtube 也會於離線時將其網站上的影片辨識成文字，但目前尚不支援直接輸入查詢詞檢索影片轉寫的方式，可以期待未來 Youtube 會開放這方面的功能。只是上述兩個例子仍要倚賴人工的轉寫，要完全只靠機器自動辨識仍不容易做到。這種語音數位內容檢索將是本論文主要的研究主軸。

語音資訊檢索系統的效能取決於其前端語音辨識系統的效能，只要可以發展出完美的語音辨識系統，能夠完全正確的把聲音轉寫為文字，那需要將文件資訊檢索的方式套用到語音辨識的文字輸出上就解決語音資訊檢索這個問題了。依此邏輯來看，與其研究語音檢索，不如去研究如何提升語音辨識的正確率，而當完美的語音辨識系統被創造出來，語音檢索這個問題也就沒什麼好研究的了。因此TREC SDR track~\cite{trec}在2000年的時候即宣稱語音檢索在廣播新聞上（BroadcastNews）是一個「已解決的問題」(solved
problem)，而在當時廣播新聞的辨識正確率已經到達90\%以上，但在自發性（Spontaneous）語音上，語音辨識正確率往往不到50\%，進而增加了在自發性語音上檢索的難度~\cite{saraclar2004lattice,mamou2006spoken}，即使最近很流行的深度學習（Deep
Learning）大幅提升語音辨識系統的能力，語音辨識仍然是個困難的問題。短時間內語音辨識錯誤似乎是不可避免的，辨識錯誤對語音檢索所帶來的傷害幾乎在這個領域的每一篇論文的導論都會提到，用以彰顯語音檢索仍是個值得研究的問題，然而卻沒有太多方法能真正突破語音辨識錯誤所帶來的限制。並且傳統的語音數位內容之語意檢索系統主要的實作方法為先將語音文件辨識為文字檔後，對文字做檢索，但在辨識之中，會遇到如詞典外詞彙
（Out Of Vocabulary）、辨識錯誤等情況，更甚者，許多語音中珍貴的資訊如韻律（Prosody）、語速（Speaking Rate）和語者特徵（Speaker Characteristic）等在辨識後就消失了，十分地可惜。

本論文因此將語音辨識的部分移除，直接在語音訊號本身進行搜索，以類神經網路直接學習語音訊號的相似性，有別於傳統的聲學模型訓練強調辨識正確率的提升，本論文所提出的移除語音辨識，直接由語音訊號進行搜索提升語音檢索的效能。

\section{研究方向}
本論文之研究方向為使用類神經網路強化檢索系統之語音資訊檢索，主要包含以下幾點:

\begin{itemize}
\itemsep -2pt %reduce space between items
  \item
	  傳統的語意檢索系統是先將語音文件辨識為文字後，將輸入的文字查詢詞進行檢索，語音辨識系統的影響會直接反應在搜尋結果上,但自動語音辨識系統（Automatically Speech Recognition）
的訓練是很昂貴的，需要大量標注完善的語料才能訓練出很好的聲學與語言模型。因此，利用類神經網路擁有有良好的推廣性（Generization）
和適應性（Adaptability）強的優點，直接對於語音信號本身搜索，避免語音辨識的錯誤。
  \item   更進一步地，由於語音文件無法跟文字文件一樣清楚分段，對於模型來說計算量是很龐大，且充斥著雜訊。因而，引進了畫重點機制
	  （Attention Mechanism）
	  ，來使模型能夠將目光注目特定的位置上，減少雜訊的干擾，進而學習出更好的向量表示
	  （Vector Representation）
  \item
	  由於學習的向量表示，會過度貼近（Overfit）訓練資料，導致推廣性的下降。使用自動編碼器（auto-encoder）\cite{vincent2008extracting,li2015hierarchical,baldi2012autoencoders}產生文字向量（word2vec）,語音向量（audio2vec）\cite{mikolov2013distributed,mikolov2013efficient,pennington2014glove,chung2016audio}，進而進行搜索。
  \item   再者，利用了成對學習法 （pairwise
	  learning）使得原本的分類問題加入了排序（Ranking）的概念，使得系統能夠更加精準的分出正確資料與錯誤資料間的差異，進而提升檢索系統。
  \item
	  最後，將上述的模型統整在一起，將語音向量當作正規化（regularization），利用畫重點機制排除多餘雜訊，提升模型的效能，使搜尋結果更加進步。

\end{itemize}

\section{章節安排}
本論文之章節安排如下：

\begin{itemize}
\itemsep -2pt %reduce space between items
  \item  第二章：介紹本論文相關背景知識。
  \item  第三章：介紹如何以類神經網路實現口述詞彙偵測。
  \item  第四章：介紹如何以畫重點機制類神經網路實現口述詞彙偵測。
  \item  第五章：介紹如何產生語音向量(audio2vec)跟文字向量(word2vec)，並應用於口述詞彙偵測
  \item  第六章：介紹如何以成對學習法訓練檢索系統。 
  \item  第七章：介紹將畫重點機制跟語音向量同時應用在口語詞彙偵測。
  \item  第八章：本論文之結論與未來研究方向。
\end{itemize}

